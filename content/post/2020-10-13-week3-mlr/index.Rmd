---
title: 'Week 3: Multiple Linear Regression'
author: Roberto Supe
date: '2020-10-12'
slug: week-3-multiple-linear-regression
categories: ["R"]
tags: ["R Markdown", "plot", "regression"]
runtime: shiny
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      echo = T,
                   cache = F,
                   message = FALSE,
                   warning = FALSE)
```


```{r}
library(PerformanceAnalytics)
library(tidyverse)
library(psych)
```

# Multiple Linear Regression

This week we will cover multiple linear regression. As in previous post and in any model that you want to do we will need to test our assumtions and follow some general steps 

Procedure to perform Multiple Linear Regression:

1. Scatterplot matrix
2. Fit multiple linear regression models
3. Calculation estimate parameters, determine their standard errors, obtain tests and confidence intervals.
4. Evaluation and explanation

Let's have our data

```{r}
set.seed(21) #so we get the same results even with a random selection
df<-data.frame(temperature=rnorm(100,mean = 25,sd=5),
               wind=rnorm(100,mean = 10,sd=1.2),
               radiation=rnorm(100,mean = 5000,sd=1000),
               bacterias=rnorm(100,mean = 50,sd=15),
               groups=c("Day","Night")) #groups
head(df,2)
```


## MLR Assumption (Independency)

Simple linear regression does not require this test because it has one independent variable. In this chapter, we test linearity and independence of observations (autocorrelation) to determine that independent variables are not too hihgly correlated.

```{r}
cor(df$temperature,df$wind) #function to test the relationship between your independent variables
```

## Normality and Linearity Assumptions 

We still need to lest the other two assumption before the model. A function that will save us time is `chart.Correlation`, we can get **Marginal Response Plots** showing correlations and the significance of those correlations (independency), histograms and distributions (normality), and bivariate scatter plot with a fitted line (linearity). Although this does not test the normal distribution you can observe with histograms the distribution of your data for all your numeric variables and make some initial stimations. 

```{r}
chart.Correlation(df[,-5], #numeric data set
                  histogram = T, #True show histograms
                  method = "pearson") #correlation method "pearson", "kendall", "spearman"
```

Similarly, with a few variables then you can use `psych::corr.test` and determine correlation and the significance. What we used for the correlation plot in [week 1](https://stats-bnu-2020.netlify.app/2020/09/23/stats-course-1week/).

```{r}
correlation.test <- psych::corr.test(df[,1:4],use = "pairwise",method = "spearman") #relationship between all your variables
my.correlation<-correlation.test$r # from the test get ($) the correlation values
p.values.correlation<-correlation.test$p # from the test get ($) the correlation p values
```

* Values close to 1 indicate that independent variables are nearly identical meaning that they do not meet the assumtions for multiple linear regression and should be treated or discharted.

## Regression fitting

```{r}
fit.lm<-lm(bacterias~.,data=df) #all the variables 
#fit.lm<-lm(bacterias~temperature+wind+radiation,data=df) same as before
summary(fit.lm)
```

Breakdown of the results elements:

* The estimates `(Estimate)` column gives you the change in y cause by each element of the regression. For `Intercept` or the value of the y when all the rest elements are 0 in this case 68.12 bacteria units on each test.Similarly, `temperature/rest of the variabels` show changes in y (population of bacteria) with one unit increase in `x`. In other words, temperature in the full model reduces bacteria population by -0.304 units for each increment in temperature if P < 0.05. Here it isnt so we can not reach that conclussion. 

* The standard error of the estimated values in the second column `Std. Error`.
* The test statistic `t value`.
* The p-value `Pr(>| t | )`, the probability of finding the given t-statistic and therefore, the calculated `estimate` by chance.


## Model test (Homoscedasticity)

We should ensure that the model fited meets this last assumption, same variance, to maintain the model. The error term needs to be the same across all values of the independent variables.

```{r}
par(mfrow=c(2,2))
plot(fit.lm)
par(mfrow=c(1,1))
```

Here we can see that the resudial erros are constant across groups and they are distributed evenly. Look at the `Normal Q-Q` (top right) which under homoscedasticity it should show points along the line.

## Leverage and extrapolation

Leverage h is a quantity solely depend on the predictors for all the cases in the data without any respect with responses. Leverage hi is called the leverage of observation i. It indicates the **“pull”** an observation has on the regression fit. 

Let's look at that pull for different points.

<iframe height="400" width="100%" frameborder="no" src=""> </iframe>

>Cases with large leverages should be inspected to make sure there are not errors in the data or other problems. We will show some methods to deal with them in the following posts

