---
title: 'Week 8: Curvature and Nonconstant Variance'
author: Roberto Supe
date: '2020-12-02'
slug: week-8-curvature-and-nonconstant-variance
categories:
  - R
tags:
  - blog
  - factors
  - factors
  - functions
  - lack of fit
  - mean
  - model comparison
  - plot
  - R Markdown
  - regression
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE,
                      echo = T,
                   cache = T,
                   message = FALSE,
                   warning = FALSE)
```

This week we will use the following packages as always, load them at the beginning of the session`r knitr::asis_output("\U1F468")``r knitr::asis_output("\U1F469")` 

```{r}
library(tidyverse)
library(car)
library(dae) #install.packages("dae")
library(car)
library(DMwR)
library(olsrr)
library(plotly)
#install.packages("package_name")
```

Linear regressions can be fit to any data, this also means that you may get similar regression results even with different data that is because you are fitting the best option to reduce the variance between your points. Let`s look at the following plots with similar regression results,$\sigma^2$, and $R^2$ but with different observations and distributions.

```{r,out.width="50%",fig.show='hold',echo=FALSE}
x=4:14
plots<-data.frame(x,
           Model=rnorm(11,mean = 3+0.5*x,sd = sqrt(1.53)),
           Curvature=c(4.0, 4.9,  6.5,  7.4,  8.2,  8.7,  9.2,  9.2,  9.1,  8.5, 8.0),
           Outlier=c(5.5,5.8,6,6.2,6.5,7,7.5,8,8.5,12,9),
           influential=c(5.2,5.6,5.8,6.2,6.5,6.6,7.1,7.2,8.6,8.8,12.5))
a=12 #size for the points
b=22 #size for the text
ggplot(plots,mapping=aes(x=x,y=Model))+
  geom_point(size=a,shape=16,colour="red")+
  geom_smooth(method = "lm")+
  labs(x="x",y="Model Y")+
  theme_test(base_size = b)
ggplot(plots,mapping=aes(x=x,y=Curvature))+
  geom_point(size=a,shape=17,colour="forestgreen")+
  geom_smooth(method = "lm")+
  labs(x="x",y="Curvature Y")+
  theme_test(base_size = b)
ggplot(plots,mapping=aes(x=x,y=Outlier))+
  geom_point(size=a,shape=18,colour="purple")+
  geom_smooth(method = "lm")+
  labs(x="x",y="Outlier Y")+
  theme_test(base_size = b)
ggplot(plots,mapping=aes(x=c(rep(8,10),19),y=influential))+
  geom_point(size=a,shape=13,colour="red")+
  geom_smooth(method = "lm")+
  labs(x="x",y="Influential Y")+
  theme_test(base_size = b)
```

Some of these plots may be extreme cases and it does not require statistical analysis to determine that one point is influencing our regression, but in the case of outliers, it may not be that obvious. Therefore, for general practice as mention in [assumptions for linear regression](https://stats-bnu-2020.netlify.app/2020/09/30/week-2-regression-and-fitting-models/) it is required that you look at the residual plots.

```{r,fig.show='hold',out.width="50%"}
fit.1<-lm(Model~x,plots)
residualPlot(fit.1)
fit.1<-lm(Curvature~x,plots)
residualPlot(fit.1)
fit.1<-lm(Outlier~x,plots)
residualPlot(fit.1)
fit.1<-lm(Curvature~c(rep(8,10),19),plots) #data in different formats
residualPlot(fit.1)
```

The best residuals sit on the `0` line (y-axis) but in general the closer to this line the better. Similarly, they are scattered along with the extent of x. **If the residual plot demonstrates either residuals or variance of residuals is not constant, then we must conclude that the model is incorrect in some respect, but we may not be able to tell if the problem is due to a misspecified mean function or a misspecified variance function.**. Now, let's use our algae data and fit a regression to analyze some indicators. 

```{r}
fit1<-lm(NO3~mxPH+Cl,data=algae)
#residuals<- fit1$residuals
plot(fit1$residuals)
```

In these data set we have one clear outlier so we should treat that observation. 
We can also plot residuals against individual predictors $x_k$ 

```{r fig.show='hold',out.width="50%"}
plot(x = fit1$model$mxPH,y = fit1$residuals)
abline(0,0)
a<-car::residualPlot(fit1,variable="Cl")
```

There are a few packages for these tests and we will have a look at a ggplot2 option.

```{r }
x<-olsrr::ols_plot_resid_fit(fit1,print_plot = F)+
  theme_bw()
ggplotly(x)
```

`olsrr` has a lot of different plots you can have a look at its [package information](https://cran.r-project.org/web/packages/olsrr/olsrr.pdf) to build great ordinary least squares regression models.


### Testing for Nonconstant Variance

Breuschâ€“Pagan test determines whether the variance of the errors from a regression model is dependent on the values of the independent variables. In that case, heteroskedasticity is present. If the test statistic has a p-value below an appropriate threshold (e.g. p < 0.05) then the null hypothesis of homoskedasticity is rejected and heteroskedasticity assumed. 

```{r}
ncvTest(fit1)
```
Here, we are concluding that there is heteroskedasticity (bad), so the variation of our terms is dependent on the values of the independent variable. There are a few common ways to fix heteroscedasticity:

1. Transform the dependent variable, simply take the log of the dependent variable.
2. Redefine the dependent variable by adding something like a *rate*.
3. Use weighted regression [week 6](https://stats-bnu-2020.netlify.app/2020/11/11/week-6-weights-and-lack-of-fit/).
4. Lastly, we can perform a different model by removing those outliers (Next Week). 















